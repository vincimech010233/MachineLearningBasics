{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "\n",
    "# Definir el entorno en una cuadrícula 5x5\n",
    "# 0: celda vacía, 1: objetivo\n",
    "grid = np.zeros((5, 5))\n",
    "grid[4, 4] = 1\n",
    "\n",
    "# Parámetros\n",
    "learning_rate = 0.1\n",
    "gamma = 0.9\n",
    "epsilon = 1.0  # para la exploración\n",
    "decay_factor = 0.99\n",
    "\n",
    "# Inicializar la tabla Q con ceros\n",
    "Q = np.zeros((25, 4))\n",
    "\n",
    "# Modelo de TensorFlow\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(4, input_shape=(25,), activation='linear')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate), loss='mse')\n",
    "\n",
    "# Entrenamiento\n",
    "for episode in range(1000):\n",
    "    state = (0, 0)\n",
    "    done = False\n",
    "    steps = 0  # Contador de pasos para este episodio\n",
    "\n",
    "    while not done:\n",
    "        steps += 1  # Incrementar el contador de pasos\n",
    "        state_index = state[0] * 5 + state[1]\n",
    "        if random.random() < epsilon:\n",
    "            # Tomar acción aleatoria\n",
    "            action = random.choice([0, 1, 2, 3])\n",
    "        else:\n",
    "            # Tomar la acción con el valor Q más alto\n",
    "            action = np.argmax(Q[state_index])\n",
    "\n",
    "        # Obtener nuevo estado y recompensa\n",
    "        new_state = list(state)\n",
    "        if action == 0:\n",
    "            new_state[0] = max(0, state[0] - 1)\n",
    "        elif action == 1:\n",
    "            new_state[0] = min(4, state[0] + 1)\n",
    "        elif action == 2:\n",
    "            new_state[1] = max(0, state[1] - 1)\n",
    "        elif action == 3:\n",
    "            new_state[1] = min(4, state[1] + 1)\n",
    "\n",
    "        reward = grid[new_state[0], new_state[1]]\n",
    "\n",
    "        # Actualizar la tabla Q usando Q-learning\n",
    "        new_state_index = new_state[0] * 5 + new_state[1]\n",
    "        target = reward + gamma * np.max(Q[new_state_index])\n",
    "        Q[state_index, action] = (1 - learning_rate) * Q[state_index, action] + learning_rate * target\n",
    "\n",
    "        # Actualizar el modelo de TensorFlow\n",
    "        target_f = Q[state_index]\n",
    "        model.fit(np.identity(25)[state_index:state_index + 1], target_f.reshape(-1, 4), epochs=1, verbose=0)\n",
    "\n",
    "        # Verificar si hemos llegado al objetivo\n",
    "        if reward == 1:\n",
    "            done = True\n",
    "            print(f\"Episodio {episode + 1} completado en {steps} pasos.\")\n",
    "        else:\n",
    "            state = tuple(new_state)\n",
    "\n",
    "    epsilon *= decay_factor\n",
    "\n",
    "# Guardar el modelo\n",
    "model.save(\"simple_grid_agent.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
