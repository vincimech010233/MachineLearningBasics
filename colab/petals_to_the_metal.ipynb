{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt  # This library is imported but not used in the code.\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data from TFRecord files.\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "IMAGE_SIZE = [224, 224]\n",
    "\n",
    "# Get the file paths of the TFRecord files.\n",
    "train_filepaths = tf.io.gfile.glob(\"/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/train/*.tfrec\")\n",
    "val_filepaths = tf.io.gfile.glob(\"/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/val/*.tfrec\")\n",
    "test_filepaths = tf.io.gfile.glob(\"/kaggle/input/tpu-getting-started/tfrecords-jpeg-224x224/test/*.tfrec\")\n",
    "\n",
    "# Function to decode the image from the TFRecord.\n",
    "def decode_image(image, label=None, image_shape=(224, 224, 3)):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)  # Decode the JPEG image.\n",
    "    image = tf.reshape(image, image_shape)  # Reshape it to the desired size.\n",
    "    if label is None:\n",
    "        return image\n",
    "    else:\n",
    "        return image, label\n",
    "\n",
    "# Function to read a single TFRecord file.\n",
    "def read_tfrecord(example, labeled=True):\n",
    "    # Define the format of the TFRecord based on whether it's labeled.\n",
    "    if labeled:\n",
    "        tfrecord_format = {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "            \"class\": tf.io.FixedLenFeature([], tf.int64),\n",
    "        }\n",
    "    else:\n",
    "        tfrecord_format = {\n",
    "            \"image\": tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "    example = tf.io.parse_single_example(example, tfrecord_format)  # Parse the TFRecord.\n",
    "    image = decode_image(example['image'])\n",
    "    if labeled:\n",
    "        label = tf.cast(example['class'], tf.int32)\n",
    "        return image, label\n",
    "    return image\n",
    "\n",
    "# Function to load a dataset from a list of TFRecord files.\n",
    "def load_dataset(filenames, labeled=True):\n",
    "    ignore_order = tf.data.Options()  # Create an option to read files in a non-deterministic order.\n",
    "    ignore_order.experimental_deterministic = False\n",
    "    dataset = tf.data.TFRecordDataset(filenames)  # Load the dataset from the files.\n",
    "    dataset = dataset.with_options(ignore_order)  # Apply the non-deterministic order option.\n",
    "    dataset = dataset.map(lambda x: read_tfrecord(x, labeled=labeled), num_parallel_calls=AUTOTUNE)  # Map the reading function.\n",
    "    return dataset\n",
    "\n",
    "# Function to configure the dataset.\n",
    "def get_dataset(filenames, labeled=True):\n",
    "    dataset = load_dataset(filenames, labeled=labeled)\n",
    "    dataset = dataset.shuffle(2048)  # Shuffle the dataset.\n",
    "    dataset = dataset.prefetch(buffer_size=AUTOTUNE)  # Prefetch data for performance.\n",
    "    dataset = dataset.batch(32)  # Batch the dataset.\n",
    "    return dataset\n",
    "\n",
    "# Create the neural network model.\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),  # Convolutional layer.\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),  # Max pooling layer.\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),  # Another convolutional layer.\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),  # Another max pooling layer.\n",
    "    tf.keras.layers.Flatten(),  # Flatten the data.\n",
    "    tf.keras.layers.Dense(128, activation='relu'),  # Dense layer.\n",
    "    tf.keras.layers.Dense(104, activation='softmax')  # Output layer.\n",
    "])\n",
    "\n",
    "# Compile the model with an optimizer, loss function, and metric.\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model.\n",
    "train_dataset = get_dataset(train_filepaths)\n",
    "val_dataset = get_dataset(val_filepaths)\n",
    "history = model.fit(train_dataset, epochs=10, validation_data=val_dataset)\n",
    "\n",
    "# Evaluate the model's performance on the validation dataset.\n",
    "val_loss, val_acc = model.evaluate(val_dataset)\n",
    "print(\"Validation accuracy: \", val_acc)\n",
    "\n",
    "# Prepare the test dataset and make predictions.\n",
    "test_dataset = get_dataset(test_filepaths, labeled=False)\n",
    "predictions = model.predict(test_dataset)\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "# Create a DataFrame for submission.\n",
    "submission_df = pd.DataFrame({'id': range(0, len(predicted_labels)), 'label': predicted_labels})\n",
    "submission_df.to_csv('/kaggle/working/submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  # Importing the matplotlib library for plotting.\n",
    "\n",
    "# Plotting accuracy and loss during training.\n",
    "plt.figure(figsize=(12, 4))  # Setting up a new figure for the plots with specified size.\n",
    "\n",
    "# First subplot for accuracy.\n",
    "plt.subplot(1, 2, 1)  # Specifies that we are creating a 1 row x 2 columns grid of plots and selecting the first plot.\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')  # Plotting the training accuracy over epochs.\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')  # Plotting the validation accuracy over epochs.\n",
    "plt.xlabel('Epoch')  # Setting the x-axis label.\n",
    "plt.ylabel('Accuracy')  # Setting the y-axis label.\n",
    "plt.legend()  # Displaying the legend, which indicates what each line on the plot represents.\n",
    "\n",
    "# Second subplot for loss.\n",
    "plt.subplot(1, 2, 2)  # Selecting the second plot in our 1 row x 2 columns grid.\n",
    "plt.plot(history.history['loss'], label='Training Loss')  # Plotting the training loss over epochs.\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')  # Plotting the validation loss over epochs.\n",
    "plt.xlabel('Epoch')  # Setting the x-axis label.\n",
    "plt.ylabel('Loss')  # Setting the y-axis label.\n",
    "plt.legend()  # Displaying the legend.\n",
    "\n",
    "plt.show()  # Displaying the plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking a small batch of images from the validation dataset for visualization.\n",
    "for img_batch, lbl_batch in val_dataset.take(1):\n",
    "    break  # Once a single batch is taken, the loop breaks.\n",
    "\n",
    "# Making predictions on that batch.\n",
    "predictions_batch = model.predict(img_batch)  # Use the model to predict the labels of the images in the batch.\n",
    "predicted_labels_batch = tf.argmax(predictions_batch, axis=1).numpy()  # Find the index with the highest prediction value for each image. This is the predicted label.\n",
    "\n",
    "# Visualizing the images alongside their true and predicted labels.\n",
    "plt.figure(figsize=(10, 10))  # Setting up a new figure for the plots with specified size.\n",
    "\n",
    "for i in range(16):  # Looping to display 16 images.\n",
    "    plt.subplot(4, 4, i + 1)  # Specifies that we are creating a 4x4 grid of plots and selecting the (i+1)th plot.\n",
    "    plt.imshow(img_batch[i].numpy().astype(\"uint8\"))  # Displaying the ith image from the batch.\n",
    "    plt.title(f\"True: {lbl_batch[i]}, Predict: {predicted_labels_batch[i]}\")  # Setting the title for each image to show the true and predicted labels.\n",
    "    plt.axis(\"off\")  # Turning off the axis labels and ticks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
